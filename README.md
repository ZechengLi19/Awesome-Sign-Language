# Awesome-Sign-Language
This repository collects the common datasets and paper list related to the research on **Sign Language**ðŸ¤Ÿ

This repository is continuously updatingðŸŽ‰

If this repository brings you some inspiration, I would be very honoredðŸ˜Š

If you have any suggestions, feel free to contact me with: lizecheng19@gmail.comðŸ“®

Additionally, if you could consider giving my repository a starðŸŒŸ, that would motivate me a lot!

## Contents
- [Datasets](#Datasets)
- [Isolated sign language recognition](#islr_2016)
- [Continue sign language recognition](#cslr_2016)
- [Sign language translation](#slt_2018)
- [Sign language production](#slp_2018)
- [Sign language retrieval](#slr_2022)
- [Unified sign language understanding](#USLU_2021)

## Popular Datasets
<a id="Datasets"></a>
- Isolated sign language recognition datasets:
  - **WLASL**: 14,289, 3,916, and 2,878 video segments in the train, dev, and test splits, respectively. [[Link](https://dxli94.github.io/WLASL/)]
  - **MSASL**: 16,054, 5,287, and 4,172 video segments in the train, dev, and test splits, respectively. [[Link](https://www.microsoft.com/en-us/research/project/ms-asl/)]
  - **NMFs-CSL**: 25,608 and 6,402 video segments in the train and test splits, respectively. [[Link](https://ustc-slr.github.io/datasets/2020_nmfs_csl/)]
  - **SLR500**: 90,000 and 35,000 video segments in the train and test splits, respectively. [[Link](http://home.ustc.edu.cn/~hagjie/)]
  - **Slovo**: 15,300 and 5,100 video segments in the train and test splits, respectively. [[Link](https://github.com/hukenovs/slovo)]
  - **GSL**: 34,995 and 3,500 video segments in the train and test splits, respectively. [[Link](https://zenodo.org/records/3941811#.XxrZXZZRU5k)]
  - **BOBSL**: 993,000, 20,000, 165,000 video segments in train, val and test splits, respectively. [[Link](https://www.robots.ox.ac.uk/~vgg/data/bobsl/)]
  - **ASL Citizen**: 40,154, 10,304, 32,941 video segments in train, val and test splits, respectively. [[Link](https://www.microsoft.com/en-us/research/project/asl-citizen/)]
  - **Auslan-Daily**: 1,800, 600, 600 video segments in train, val and test splits, respectively. [[Link](https://uq-cvlab.github.io/Auslan-Daily-Dataset/)]

- Continue sign language recognition datasets:
  - **Phoenix-2014**: 5,672, 540 and 629 video segments in the train, dev, and test splits, respectively. [[Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/)]
  - **Phoenix-2014T**: 7,096, 519 and 642 video segments in train, dev and test splits, respectively. [[Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)]
  - **CSL-Daily**: 18,401, 1,077 and 1,176 video segments in train, dev and test splits, respectively. [[Link](http://home.ustc.edu.cn/~zhouh156/dataset/csl-daily/)]
  - **GSL**: 8,189, 1,063 and 1,043 video segments in train, dev and test splits, respectively. [[Link](https://zenodo.org/records/3941811#.XxrZXZZRU5k)]
  - **TVB-HKSL-News**: 6,516, 322 and 322 video segments in train, dev and test splits, respectively. [[Link](https://tvb-hksl-news.github.io/)]
  
- Sign language translation datasets:
  - **Phoenix-2014T**: 7,096, 519 and 642 video segments in
train, dev and test splits, respectively. [[Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)]
  - **TVB-HKSL-News**: 6,516, 322 and 322 video segments in train, dev and test splits, respectively. [[Link](https://tvb-hksl-news.github.io/)]
  - **CSL-Daily**: 18,401, 1,077 and 1,176 video segments in train, dev
and test splits, respectively. [[Link](http://home.ustc.edu.cn/~zhouh156/dataset/csl-daily/)]
  - **OpenASL**: 96,476, 966 and 975 video segments in train, val and test splits, respectively. [[Link](https://github.com/chevalierNoir/OpenASL/)]
  - **How2Sign**: 31,128, 1,741, 2,322 video segments in train, val and test splits, respectively. [[Link](https://how2sign.github.io/)]
  - **BOBSL**: 993,000, 20,000, 165,000 video segments in train, val and test splits, respectively. [[Link](https://www.robots.ox.ac.uk/~vgg/data/bobsl/)]
  - **Auslan-Daily Communication**: 12,441, 800, 800 video segments in train, val and test splits, respectively. [[Link](https://uq-cvlab.github.io/Auslan-Daily-Dataset/)]
  - **Auslan-Daily News**: 9,665, 700, 700 video segments in train, val and test splits, respectively. [[Link](https://uq-cvlab.github.io/Auslan-Daily-Dataset/)]

## Paper List
### Isolated sign language recognition
### <a id="islr_2016">2016</a>
  - **Iterative Reference Driven Metric Learning for Signer Independent Isolated Sign**. *ECCV 2016*. [[Paper](http://vipl.ict.ac.cn/uploadfile/upload/2018112115134267.pdf)]

### <a id="islr_2019">2019</a>
  - **Skeleton-Based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module**. *AAAI 2019*. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/4878)]

### <a id="islr_2020">2020</a>
  - **Transferring Cross-Domain Knowledge for Video Sign Language Recognition**. *CVPR 2020*. [[Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Transferring_Cross-Domain_Knowledge_for_Video_Sign_Language_Recognition_CVPR_2020_paper.html)]
  - **BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues**. *ECCV 2020*. [[Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1279_ECCV_2020_paper.php)]
  - **Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison**. *WACV 2020*. [[Paper](https://openaccess.thecvf.com/content_WACV_2020/html/Li_Word-level_Deep_Sign_Language_Recognition_from_Video_A_New_Large-scale_WACV_2020_paper.html)][[Code](https://github.com/dxli94/WLASL)] 
  - **FineHand: Learning Hand Shapes for American Sign Language Recognition**. *FG 2020*. [[Paper](https://ieeexplore.ieee.org/document/9320289)]

### <a id="islr_2021">2021</a>
  - **Hand-Model-Aware Sign Language Recognition**. *AAAI 2021*. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/16247)]
  - **Global-Local Enhancement Network for NMF-Aware Sign Language Recognition**. *TOMM 2021*. [[Paper](https://dl.acm.org/doi/10.1145/3436754)]
  - **Hand Pose Guided 3D Pooling for Word-level Sign Language Recognition**. *WACV 2021*. [[Paper](https://openaccess.thecvf.com/content/WACV2021/html/Hosain_Hand_Pose_Guided_3D_Pooling_for_Word-Level_Sign_Language_Recognition_WACV_2021_paper.html)]
  - **Pose-based Sign Language Recognition using GCN and BERT**. *WACVW 2021*. [[Paper](https://openaccess.thecvf.com/content/WACV2021W/HBU/html/Tunga_Pose-Based_Sign_Language_Recognition_Using_GCN_and_BERT_WACVW_2021_paper.html)]
  - **Skeleton Aware Multi-modal Sign Language Recognition**. *CVPRW 2021*. [[Paper](https://arxiv.org/pdf/2103.08833.pdf)][[Code](https://github.com/jackyjsy/CVPR21Chal-SLR)]
  - **Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble**. *Arxiv 2021*. [[Paper](https://arxiv.org/pdf/2110.06161.pdf)][[Code](https://github.com/jackyjsy/SAM-SLR-v2)]
  - **SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition**. *ICCV 2021*. [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Hu_SignBERT_Pre-Training_of_Hand-Model-Aware_Representation_for_Sign_Language_Recognition_ICCV_2021_paper.html)]

### <a id="islr_2023">2023</a>
  - **Isolated Sign Language Recognition based on Tree Structure Skeleton Images**. *CVPRW 2023*. [[Paper](https://arxiv.org/pdf/2304.05403.pdf)][[Code](https://github.com/davidlainesv/SL-TSSI-DenseNet)]
  - **Natural Language-Assisted Sign Language Recognition**. *CVPR 2023*. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zuo_Natural_Language-Assisted_Sign_Language_Recognition_CVPR_2023_paper.html)][[Code](https://github.com/FangyunWei/SLRT/tree/main/NLA-SLR)]
  - **Human Part-wise 3D Motion Context Learning for Sign Language Recognition**. *ICCV 2023*. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Human_Part-wise_3D_Motion_Context_Learning_for_Sign_Language_Recognition_ICCV_2023_paper.pdf)]
  - **BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization**. *AAAI 2023*. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/25470)]
  - **Self-Supervised Representation Learning with Spatial-Temporal Consistency for Sign Language Recognition**. *TIP 2023*. [[Paper](https://arxiv.org/pdf/2406.10501)][[Code](https://github.com/sakura2233565548/Self-Supervised-Representation-Learning-with-Spatial-Temporal-Consistency-for-SLR)]

### <a id="islr_2024">2024</a>
  - **MASA: Motion-aware Masked Autoencoder with Semantic Alignment for Sign Language Recognition**. *TCSVT 2024*. [[Paper](https://arxiv.org/pdf/2405.20666)][[Code](https://github.com/sakura2233565548/MASA)]

[//]: # (### <a id="islr_2024">2024</a>)

### Continue sign language recognition
### <a id="cslr_2016">2016</a>
  - **Deep Sign: Hybrid CNN-HMM for Continuous Sign Language Recognition**. *BMVC 2016*. [[Paper](https://bmva-archive.org.uk/bmvc/2016/papers/paper136/index.html)]

### <a id="cslr_2017">2017</a>
  - **SubUNets: End-To-End Hand Shape and Continuous Sign Language Recognition**. *ICCV 2017*. [[Paper](https://openaccess.thecvf.com/content_iccv_2017/html/Camgoz_SubUNets_End-To-End_Hand_ICCV_2017_paper.html)]
  - **Recurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization**. *CVPR 2017*. [[Paper](https://openaccess.thecvf.com/content_cvpr_2017/html/Cui_Recurrent_Convolutional_Neural_CVPR_2017_paper.html)]

### <a id="cslr_2018">2018</a>
  - **Deep Sign: Enabling Robust Statistical Continuous Sign Language Recognition via Hybrid CNN-HMMs**. *IJCV 2018*. [[Paper](https://link.springer.com/article/10.1007/s11263-018-1121-3)]

### <a id="cslr_2019">2019</a>
  - **Iterative Alignment Network for Continuous Sign Language Recognition**. *CVPR 2019*. [[Paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Pu_Iterative_Alignment_Network_for_Continuous_Sign_Language_Recognition_CVPR_2019_paper.html)]
  - **Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign Language Videos**. *TPAMI 2019*. [[Paper](https://ieeexplore.ieee.org/document/8691602/)]

### <a id="cslr_2020">2020</a>
  - **Boosting Continuous Sign Language Recognition via Cross Modality Augmentation**. *ACM MM 2020*. [[Paper](https://dl.acm.org/doi/abs/10.1145/3394171.3413931)]
  - **Stochastic Fine-grained Labeling of Multi-state Sign Glosses for Continuous Sign Language Recognition**. *ECCV 2020*. [[Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2527_ECCV_2020_paper.php)]
  - **Fully Convolutional Networks for Continuous Sign Language Recognition**. *ECCV 2020*. [[Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4763_ECCV_2020_paper.php)]
  - **Spatial-Temporal Multi-Cue Network for Continuous Sign Language Recognition**. *AAAI 2020*. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/7001)]

### <a id="cslr_2021">2021</a>
  - **Visual Alignment Constraint for Continuous Sign Language Recognition**. *ICCV 2021*. [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Min_Visual_Alignment_Constraint_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.html)][[Code](https://github.com/ycmin95/VAC_CSLR)] 
  - **Self-Mutual Distillation Learning for Continuous Sign Language Recognition**. *ICCV 2021*. [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Hao_Self-Mutual_Distillation_Learning_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.html)]

### <a id="cslr_2022">2022</a>
  - **Signing Outside the Studio: Benchmarking Background Robustness for Continuous Sign Language Recognition**. *BMVC 2022*. [[Paper](https://bmvc2022.mpi-inf.mpg.de/322/)][[Code](https://github.com/art-jang/Signing-Outside-the-Studio)]
  - **Temporal Lift Pooling for Continuous Sign Language Recognition**. *ECCV 2022*. [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/160_ECCV_2022_paper.php)][[Code](https://github.com/hulianyuyy/Temporal-Lift-Pooling)]
  - **Deep Radial Embedding for Visual Sequence Learning**. *ECCV 2022*. [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5670_ECCV_2022_paper.php)]
  - **C2SLR: Consistency-Enhanced Continuous Sign Language Recognition**. *CVPR 2022*. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zuo_C2SLR_Consistency-Enhanced_Continuous_Sign_Language_Recognition_CVPR_2022_paper.html)]

### <a id="cslr_2023">2023</a>
  - **AdaBrowse: Adaptive Video Browser for Efficient Continuous Sign Language Recognition**. *ACM MM 2023*. [[Paper](https://dl.acm.org/doi/10.1145/3581783.3611745)][[Code](https://github.com/hulianyuyy/AdaBrowse)]
  - **CoSign: Exploring Co-occurrence Signals in Skeleton-based Continuous Sign Language Recognition**. *ICCV 2023*. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Jiao_CoSign_Exploring_Co-occurrence_Signals_in_Skeleton-based_Continuous_Sign_Language_Recognition_ICCV_2023_paper.html)]
  - **Improving Continuous Sign Language Recognition with Cross-Lingual Signs**. *ICCV 2023*. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Improving_Continuous_Sign_Language_Recognition_with_Cross-Lingual_Signs_ICCV_2023_paper.html)]
  - **C2ST: Cross-modal Contextualized Sequence Transduction for Continuous Sign Language Recognition**. *ICCV 2023*. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_C2ST_Cross-Modal_Contextualized_Sequence_Transduction_for_Continuous_Sign_Language_Recognition_ICCV_2023_paper.html)]
  - **CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment**. *CVPR 2023*. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.html)][[Code](https://github.com/binbinjiang/CVT-SLR)]
  - **Continuous Sign Language Recognition with Correlation Network**. *CVPR 2023*. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.html)][[Code](https://github.com/hulianyuyy/CorrNet)]
  - **Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition**. *CVPR 2023*. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Guo_Distilling_Cross-Temporal_Contexts_for_Continuous_Sign_Language_Recognition_CVPR_2023_paper.html)]
  - **Self-Emphasizing Network for Continuous Sign Language Recognition**. *AAAI 2023*. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/25164)][[Code](https://github.com/hulianyuyy/SEN_CSLR)]
  - **Prior-Aware Cross Modality Augmentation Learning for Continuous Sign Language Recognition**. *TMM 2023*. [[Paper](https://ieeexplore.ieee.org/document/10105511)]

### <a id="cslr_2024">2024</a>
  - **Towards Online Continuous Sign Language Recognition and Translation**. *EMNLP 2024*. [[Paper](https://aclanthology.org/2024.emnlp-main.619.pdf)][[Code](https://github.com/FangyunWei/SLRT)]

### <a id="cslr_2025">2025</a>
  - **KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe and 3D to 1D Knowledge Distillation**. *AAAI 2025*. [[Paper](https://arxiv.org/pdf/2501.02321)]


### Sign language translation
### <a id="slt_2018">2018</a>
  - **Neural Sign Language Translation**. *CVPR 2018*. [[Paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Camgoz_Neural_Sign_Language_CVPR_2018_paper.html)][[Code](https://github.com/neccam/nslt)]

### <a id="slt_2020">2020</a>
  - **Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation**. *CVPR 2020*. [[Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Camgoz_Sign_Language_Transformers_Joint_End-to-End_Sign_Language_Recognition_and_Translation_CVPR_2020_paper.html)][[Code](https://github.com/neccam/slt)]
  - **TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation**. *NeurIPS 2020*. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2020/hash/8c00dee24c9878fea090ed070b44f1ab-Abstract.html)][[Code](https://github.com/verashira/TSPNet)]
  - **Neural Sign Language Translation by Learning Tokenization**. *FG 2020*. [[Paper](https://ieeexplore.ieee.org/document/9320278?denied=)]

### <a id="slt_2021">2021</a>
  - **Spatial-Temporal Multi-Cue Network for Sign Language Recognition and Translation**. *TMM 2021*. [[Paper](https://ieeexplore.ieee.org/document/9354538)]
  - **Conditional Sentence Generation and Cross-Modal Reranking for Sign Language Translation**. *TMM 2021*. [[Paper](https://ieeexplore.ieee.org/document/9447976)]
  - **How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language**. *CVPR 2021*. [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Duarte_How2Sign_A_Large-Scale_Multimodal_Dataset_for_Continuous_American_Sign_Language_CVPR_2021_paper.html)][[Project](https://how2sign.github.io/)]
  - **Improving Sign Language Translation with Monolingual Data by Sign Back-Translation**. *CVPR 2021*. [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Model-Aware_Gesture-to-Gesture_Translation_CVPR_2021_paper.html)]
  - **Skeleton-Aware Neural Sign Language Translation**. *ACM MM 2021*. [[Paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475577)][[Code](https://github.com/SignLanguageCode/SANet)]
  - **SimulSLT: End-to-End Simultaneous Sign Language Translation**. *ACM MM 2021*. [[Paper](https://arxiv.org/abs/2112.04228)][[Code](https://github.com/Robert0125/SimulSLT)]

### <a id="slt_2022">2022</a>
  - **Prior Knowledge and Memory Enriched Transformer for Sign Language Translation**. *ACL 2022*. [[Paper](https://aclanthology.org/2022.findings-acl.297/)][[Code](https://github.com/hugddygff/PET)]
  - **Open-Domain Sign Language Translation Learned from Online Video**. *EMNLP 2022*. [[Paper](https://aclanthology.org/2022.emnlp-main.427/)][[Code](https://github.com/chevalierNoir/OpenASL)]
  - **Automatic Gloss-level Data Augmentation for Sign Language Translation**. *LREC 2022*. [[Paper](https://aclanthology.org/2022.lrec-1.734.pdf)]
  - **A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation**. *CVPR 2022*. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_A_Simple_Multi-Modality_Transfer_Learning_Baseline_for_Sign_Language_Translation_CVPR_2022_paper.html)][[Code](https://github.com/FangyunWei/SLRT/tree/main/TwoStreamNetwork)]
  - **MLSLT: Towards Multilingual Sign Language Translation**. *CVPR 2022*. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Yin_MLSLT_Towards_Multilingual_Sign_Language_Translation_CVPR_2022_paper.html)][[Code](https://github.com/MLSLT/SP-10)]
  - **Two-Stream Network for Sign Language Recognition and Translation**. *NeurIPS 2022*. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/6cd3ac24cdb789beeaa9f7145670fcae-Abstract-Conference.html)][[Code](https://github.com/FangyunWei/SLRT/tree/main/TwoStreamNetwork)]
  - **Sign Language Translation With Hierarchical Spatio-Temporal Graph Neural Network**. *WACV 2022*. [[Paper](https://openaccess.thecvf.com/content/WACV2022/html/Kan_Sign_Language_Translation_With_Hierarchical_Spatio-Temporal_Graph_Neural_Network_WACV_2022_paper.html)]
  - **Sign Language Translation based on Transformers for the How2Sign Dataset**. *Report 2022*. [[Paper](https://imatge.upc.edu/web/sites/default/files/pub/xCabot22.pdf)]

### <a id="slt_2023">2023</a>
  - **SLTUNET: A Simple Unified Model for Sign Language Translation**. *ICLR 2023*. [[Paper](https://openreview.net/forum?id=EBS4C77p_5S)][[Code](https://github.com/bzhangGo/sltunet)]
  - **Gloss-Free End-to-End Sign Language Translation**. *ACL 2023*. [[Paper](https://aclanthology.org/2023.acl-long.722/)][[Code](https://github.com/HenryLittle/GloFE)]
  - **Neural Machine Translation Methods for Translating Text to Sign Language Glosses**. *ACL 2023*. [[Paper](https://aclanthology.org/2023.acl-long.700/)]
  - **Considerations for meaningful sign language machine translation based on glosses**. *ACL 2023*. [[Paper](https://aclanthology.org/2023.acl-short.60/)]
  - **ISLTranslate: Dataset for Translating Indian Sign Language**. *ACL 2023*. [[Paper](https://aclanthology.org/2023.findings-acl.665/)][[Code](https://github.com/Exploration-Lab/ISLTranslate)]
  - **Cross-modality Data Augmentation for End-to-End Sign Language Translation**. *EMNLP 2023*. [[Paper](https://arxiv.org/pdf/2305.11096.pdf)][[Code](https://github.com/Atrewin/SignXmDA)]
  - **YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus**. *NeurIPS 2023*. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/5c61452daca5f0c260e683b317d13a3f-Paper-Datasets_and_Benchmarks.pdf)]
  - **Sign Language Translation from Instructional Videos**. *CVPRW 2023*. [[Paper](https://openaccess.thecvf.com/content/CVPR2023W/WiCV/papers/Tarres_Sign_Language_Translation_from_Instructional_Videos_CVPRW_2023_paper.pdf)][[Project](https://imatge-upc.github.io/slt_how2sign_wicv2023/)][[Code](https://github.com/imatge-upc/slt_how2sign_wicv2023)]
  - **Gloss Attention for Gloss-free Sign Language Translation**. *CVPR 2023*. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.html)][[Code](https://github.com/YinAoXiong/GASLT)]
  - **Sign Language Translation with Iterative Prototype**. *ICCV 2023*. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Sign_Language_Translation_with_Iterative_Prototype_ICCV_2023_paper.html)]
  - **Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining**. *ICCV 2023*. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Gloss-Free_Sign_Language_Translation_Improving_from_Visual-Language_Pretraining_ICCV_2023_paper.html)][[Code](https://github.com/zhoubenjia/GFSLT-VLP)]

### <a id="slt_2024">2024</a>
  - **Sign Language Translation with Sentence Embedding Supervision**. *ACL 2024*. [[Paper](https://aclanthology.org/2024.acl-short.40.pdf)]
  - **Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation**. *ICLR 2024*. [[Paper](https://openreview.net/forum?id=LqaEEs3UxU)][[Code](https://github.com/ryanwongsa/Sign2GPT/tree/main)]
  - **Conditional Variational Autoencoder for Sign Language Translation with Cross-Modal Alignment**. *AAAI 2024*. [[Paper](https://arxiv.org/pdf/2312.15645.pdf)][[Code](https://github.com/rzhao-zhsq/CV-SLT)]
  - **Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation**. *LREC-COLING 2024*. [[Paper](https://arxiv.org/pdf/2403.12556.pdf)]
  - **Towards Privacy-Aware Sign Language Translation at Scale**. *ACL 2024*. [[Paper](https://arxiv.org/pdf/2402.09611)][[Code](https://github.com/facebookresearch/ssvp_slt)]
  - **LLMs are Good Sign Language Translators**. *CVPR 2024*. [[Paper](https://arxiv.org/pdf/2404.00925)]
  - **Improving Gloss-free Sign Language Translation by Reducing Representation Density**. *NeurIPS 2024*. [[Paper](https://openreview.net/forum?id=FtzLbGoHW2)][[Code](https://github.com/JinhuiYE/SignCL)]
  - **Scaling Sign Language Translation**. *NeurIPS 2024*. [[Paper](https://openreview.net/forum?id=M80WgiO2Lb)]
  - **Visual Alignment Pre-training for Sign Language Translation**. *ECCV 2024*. [[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05894.pdf)]
  - **EvSign: Sign Language Recognition and Translation with Streaming Events**. *ECCV 2024*. [[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00799.pdf)][[Project](https://zhang-pengyu.github.io/EVSign/)]


### <a id="slt_2025">2025</a>
  - **Improvement in Sign Language Translation Using Text CTC Alignment**. *COLING 2025*. [[Paper](https://arxiv.org/pdf/2412.09014)][[Code](https://github.com/Claire874/TextCTC-SLT)]
  - **Uni-Sign: Toward Unified Sign Language Understanding at Scale**. *ICLR 2025*. [[Paper](https://openreview.net/pdf?id=0Xt7uT04cQ)][[Code](https://github.com/ZechengLi19/Uni-Sign)]
  - **Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues**. *CVPR 2025*. [[Paper](https://arxiv.org/pdf/2501.09754)][[Project](https://www.robots.ox.ac.uk/~vgg/research/litfic/)]


### Sign language production
### <a id="slp_2018">2018</a>
  - **GestureGAN for Hand Gesture-to-Gesture Translation in the Wild**. *ACM MM 2018*. [[Paper](https://dl.acm.org/doi/abs/10.1145/3240508.3240704)]

### <a id="slp_2020">2020</a>
  - **Neural Sign Language Synthesis: Words Are Our Glosses**. *WACV 2020*. [[Paper](https://openaccess.thecvf.com/content_WACV_2020/papers/Zelinka_Neural_Sign_Language_Synthesis_Words_Are_Our_Glosses_WACV_2020_paper.pdf)]
  - **Adversarial Training for Multi-Channel Sign Language Production**. *BMVC 2020*. [[Paper](https://arxiv.org/abs/2008.12405)][[Code](https://github.com/BenSaunders27/ProgressiveTransformersSLP)]
  - **Progressive Transformers for End-to-End Sign Language Production**. *ECCV 2020*. [[Paper](https://arxiv.org/pdf/2004.14874.pdf)][[Code](https://github.com/BenSaunders27/ProgressiveTransformersSLP)]
  - **Text2Sign: Towards Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks**. *IJCV 2020*. [[Paper](https://link.springer.com/article/10.1007/s11263-019-01281-2#citeas)]

### <a id="slp_2021">2021</a>
  - **Towards Fast and High-Quality Sign Language Production**. *ACM MM 2021*. [[Paper](https://dl.acm.org/doi/10.1145/3474085.3475463)]
  - **Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives**. *ICCV 2021*. [[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Saunders_Mixed_SIGNals_Sign_Language_Production_via_a_Mixture_of_Motion_ICCV_2021_paper.pdf)]
  - **Model-Aware Gesture-to-Gesture Translation**. *CVPR 2021*. [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Model-Aware_Gesture-to-Gesture_Translation_CVPR_2021_paper.html)]
  - **Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks**. *IJCV 2021*. [[Paper](https://link.springer.com/article/10.1007/s11263-021-01457-9)][[Code](https://github.com/BenSaunders27/ProgressiveTransformersSLP)]

### <a id="slp_2022">2022</a>
  - **Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production**. *CVPR 2022*. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Saunders_Signing_at_Scale_Learning_to_Co-Articulate_Signs_for_Large-Scale_Photo-Realistic_CVPR_2022_paper.html)]

### <a id="slp_2024">2024</a>
  - **Sign Language Production with Latent Motion Transformer**. *WACV 2024*. [[Paper](https://arxiv.org/pdf/2312.12917.pdf)]
  - **SignAvatar: Sign Language 3D Motion Reconstruction and Generation**. *FG 2024*. [[Paper](https://arxiv.org/pdf/2405.07974)][[Project](https://dongludeeplearning.github.io/SignAvatar.html)]
  - **Select and Reorder: A Novel Approach for Neural Sign Language Production**. *LREC-COLING 2024*. [[Paper](https://arxiv.org/pdf/2404.11532)][[Project](https://t2sgpt-demo.yinaoxiong.cn/)]
  - **T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text**. *ACL 2024*. [[Paper](https://arxiv.org/pdf/2406.07119)][[Project](https://t2sgpt-demo.yinaoxiong.cn/)]
  - **G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model**. *AAAI 2024*. [[Paper](https://arxiv.org/pdf/2208.09141)][[Project](https://slpdiffusier.github.io/g2p-ddm/)]
  - **SignGen: End-to-End Sign Language Video Generation with Latent Diffusion**. *ECCV 2024*. [[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06988.pdf)][[Code](https://github.com/mingtiannihao/SignGen)]
  - **A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars**. *ECCV 2024*. [[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06499.pdf)][[Code](https://github.com/FangyunWei/SLRT)]

### <a id="slp_2025">2025</a>
  - **Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production**. *AAAI 2025*. [[Paper](https://arxiv.org/pdf/2412.13609)][[Code](https://github.com/NaVi-start/Sign-IDD)]
  - **Discrete to Continuous: Generating Smooth Transition Poses from Sign Language Observations**. *CVPR 2025*. [[Paper](https://arxiv.org/pdf/2411.16810)]


### Sign language retrieval
### <a id="slr_2022">2022</a>
  - **Sign Language Video Retrieval with Free-Form Textual Queries**. *CVPR 2022*. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Duarte_Sign_Language_Video_Retrieval_With_Free-Form_Textual_Queries_CVPR_2022_paper.pdf)][[Project](https://imatge-upc.github.io/sl_retrieval/)]

### <a id="slr_2023">2023</a>
  - **CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning**. *CVPR 2023*. [[Paper](https://arxiv.org/pdf/2303.12793.pdf)][[Code](https://github.com/FangyunWei/SLRT)]

### <a id="slr_2024">2024</a>
  - **SEDS: Semantically Enhanced Dual-Stream Encoder for Sign Language Retrieval**. *ACM MM 2024*. [[Paper](https://arxiv.org/pdf/2407.16394)][[Code](https://github.com/longtaojiang/SEDS)]
  - **Uncertainty-aware Sign Language Video Retrieval with Probability Distribution Modeling**. *ECCV 2024*. [[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06074.pdf)][[Code](https://github.com/xua222/UPRet)]

### Unified sign language understanding
### <a id="USLU_2023">2023</a>
  - **SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding**. *TPAMI 2023*. [[Paper](https://ieeexplore.ieee.org/document/10109128)][[Project](https://signbert-zoo.github.io/)]

### <a id="USLU_2024">2024</a>
  - **Scaling up Multimodal Pre-training for Sign Language Understanding**. *ArXiv 2024*. [[Paper](https://arxiv.org/pdf/2408.08544)]

### <a id="USLU_2025">2025</a>
  - **Uni-Sign: Toward Unified Sign Language Understanding at Scale**. *ICLR 2025*. [[Paper](https://openreview.net/pdf?id=0Xt7uT04cQ)][[Code](https://github.com/ZechengLi19/Uni-Sign)]

